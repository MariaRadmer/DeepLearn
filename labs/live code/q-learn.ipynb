{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iMs6H6nlc7H"
      },
      "source": [
        "\n",
        "# Q Learning with OpenAI Taxi-v3 üïπÔ∏èüöï\n",
        "\n",
        "\n",
        "In this Notebook, we'll implement an agent that plays OpenAI Taxi-v3.\n",
        "\n",
        "The goal of this game is that our agent must pick up the passenger at one location and drop him off to the goal as fast as possible.\n",
        "\n",
        "There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another.\n",
        "\n",
        "* You receive +20 points for a successful dropoff\n",
        "* Lose 1 point for every timestep it takes.\n",
        "* There is also a 10 point penalty for illegal pick-up and drop-off actions (if you don't drop the passenger in one of the 3 other locations)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSYdY5GEly4z"
      },
      "source": [
        "## Step 0: Import libs\n",
        "\n",
        "\n",
        "* Numpy for our Qtable\n",
        "* OpenAI Gym for our Taxi Environment\n",
        "* Random to generate random numbers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4V_yqRnLc4U7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import pygame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5uEXNcRl5ep"
      },
      "source": [
        "## Step 1: Initiate the environment\n",
        "\n",
        "\n",
        "* Here we'll create the Taxi environment.\n",
        "* OpenAI Gym is a library composed of many environments that we can use to train our agents.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNkGl_7rdZUL",
        "outputId": "4c30cf12-c49d-45a7-a9b5-4593ca8898d1"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "from pygame.locals import *\n",
        "import sys, time, random\n",
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import pygame\n",
        "from pynput import keyboard\n",
        "\n",
        "pygame.init()\n",
        "env = gym.make(\"Taxi-v3\", render_mode='human')\n",
        "env.reset()\n",
        "env.render()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93dq5V1bmGYr"
      },
      "source": [
        "## Step 2: Create a Q-table and init it with zeros\n",
        "\n",
        "\n",
        "* Now, we'll create our Q-table, to know how much rows (states) and columns (actions) we need, we need to calculate the `action_size` and the `state_size`\n",
        "* OpenAI Gym provides us a way to do that: `env.action_space.n` and `env.observation_space.n`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWI6T9gJdkze",
        "outputId": "30f540f8-ea7c-4ab4-f225-5823fc1bbe1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action size  6\n",
            "State size  500\n",
            "Discrete(6)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "action_size = env.action_space.n\n",
        "print(\"Action size \", action_size)\n",
        "\n",
        "state_size = env.observation_space.n\n",
        "print(\"State size \", state_size)\n",
        "\n",
        "obs = env.reset()\n",
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOVmgktPdr68",
        "outputId": "efeedd8d-9acf-4889-d60d-c035a84c5251"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1dtCv_pmfMr"
      },
      "source": [
        "## Step 3: Create the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hx9Zig-3dtaQ"
      },
      "outputs": [],
      "source": [
        "total_episodes = 50000        # Total episodes\n",
        "total_test_episodes = 5     # Total test episodes\n",
        "max_steps = 99                # Max steps per episode\n",
        "\n",
        "learning_rate = 0.7           # Learning rate\n",
        "gamma = 0.618                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability \n",
        "decay_rate = 0.01             # Exponential decay rate for exploration prob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aSlBGIHmmG2"
      },
      "source": [
        "## Step 4: The Q learning algorithm\n",
        "\n",
        "* Here we implement the Q-learning algorithm that will train the taxi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WplTBEYidyCL"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\mari9\\Desktop\\DeepLearn\\labs\\lab4.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab4.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m new_state, reward, done, _, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab4.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab4.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m qtable[state, action] \u001b[39m=\u001b[39m qtable[state, action] \u001b[39m+\u001b[39m learning_rate \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab4.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m                             np\u001b[39m.\u001b[39mmax(qtable[new_state, :]) \u001b[39m-\u001b[39m qtable[state, action])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab4.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Our new state is state\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab4.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m state \u001b[39m=\u001b[39m new_state\n",
            "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ],
      "source": [
        "# 2 For life or until learning is stopped\n",
        "for episode in range(total_episodes):\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_return = 0\n",
        "    for step in range(max_steps):\n",
        "        # 3. Choose an action a in the current world state (s)\n",
        "        ## First we randomize a number\n",
        "        exp_exp_tradeoff = random.uniform(0,1)\n",
        "        \n",
        "        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        # Else doing a random choice --> exploration\n",
        "        else:\n",
        "            action = env.action_space.sample()\n",
        "        \n",
        "        # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "        new_state, reward, done, _, info = env.step(action)\n",
        "\n",
        "        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * \n",
        "                                    np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "                \n",
        "        # Our new state is state\n",
        "        state = new_state\n",
        "        \n",
        "        total_return += reward\n",
        "        # If done : finish episode\n",
        "        if done == True: \n",
        "            break\n",
        "    print(f\"episode return {total_return}\")\n",
        "    \n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ9SAixtmv-1"
      },
      "source": [
        "## Step 5: Use our Q-table to define the Taxi driving policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bJQB5Nrd8W1",
        "outputId": "cdba12b6-1057-48ad-ac58-b86d8a0f9795"
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "rewards = []\n",
        "\n",
        "for episode in range(total_test_episodes):\n",
        "    state = env.reset()\n",
        "    ##img = env.render(mode=\"rgb_array\")\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards = 0\n",
        "    # print(\"****************************************************\")\n",
        "    # print(\"EPISODE \", episode)\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        ## Render images and save into an array\n",
        "        #images.append(img)\n",
        "        img = env.render()\n",
        "\n",
        "        action = np.argmax(qtable[state,:])\n",
        "        \n",
        "        new_state, reward, done, info = env.step(action)\n",
        "        \n",
        "        total_rewards += reward\n",
        "        \n",
        "        if done:\n",
        "            rewards.append(total_rewards)\n",
        "            #print (\"Score\", total_rewards)\n",
        "            break\n",
        "        state = new_state\n",
        "env.close()\n",
        "#imageio.mimwrite('./taxi.gif',\n",
        "#                [np.array(img) for i, img in enumerate(images) if i%2 == 0],\n",
        "#                fps=29)\n",
        "print (\"Score over time: \" +  str(sum(rewards)/total_test_episodes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XW6WhZjgVqA"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "gifPath = Path(\"./taxi.gif\")\n",
        "# Display GIF in Jupyter, CoLab, IPython\n",
        "with open(gifPath,'rb') as f:\n",
        "    display.Image(data=f.read(), format='png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUaWxEBKz1UQ",
        "outputId": "351588f4-dba1-4011-fe70-7514d2446132"
      },
      "outputs": [],
      "source": [
        "print(qtable[1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a9cc3d5033db7a69cc0cff3732b1a7e010a67b5ff3241f0937560160e7695f0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
