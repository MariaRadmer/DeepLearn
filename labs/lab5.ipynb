{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deep Reinforcement Learning\n",
    "In this lab we will implement and train an agent that uses deep learning to play balance the stick in `CartPole-v1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup\n",
    "----\n",
    "We import useful packages: `gym`, `torch` stuff, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib as plt\n",
    "\n",
    "from collections import deque  # for memory\n",
    "from tqdm import tqdm          # for progress bar\n",
    "import numpy as np\n",
    "\n",
    "from copy import deepcopy\n",
    "import random\n",
    "from turtle import done, st\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "How the game looks (without our agent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"env = gym.make('CartPole-v1', render_mode='human')\\nfor _ in tqdm(range(10)):\\n    state, _ = env.reset()\\n    done = False\\n    while not done:\\n        action = env.action_space.sample()\\n        next_state, reward, done, _, _ = env.step(action)\\nenv.close()\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"env = gym.make('CartPole-v1', render_mode='human')\n",
    "for _ in tqdm(range(10)):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "env.close()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## DQN Algorithm\n",
    "-------------\n",
    "We train a policy that tries to maximize the discounted,\n",
    "cumulative reward\n",
    "$R_{t_0} = \\sum_{t=t_0}^{\\infty} \\gamma^{t - t_0} r_t$, where\n",
    "$R_{t_0}$ is *return*. The discount, $\\gamma$ is the discount, between $0$ and $1$\n",
    "\n",
    "\n",
    "Q-learning tries to find a function\n",
    "$Q^*: State \\times Action \\rightarrow \\mathbb{R}$, maximizes rewards:\n",
    "\n",
    "$\\begin{align}\\pi^*(s) = \\arg\\!\\max_a \\ Q^*(s, a)\\end{align}$\n",
    "\n",
    "However, we don't know $Q^*$. So, we use neural network as a approximators, we can simply create one and train it to resemble $Q^*$.\n",
    "\n",
    "For our training update rule, we'll use a fact that every $Q$\n",
    "function for some policy obeys the Bellman equation:\n",
    "\n",
    "$\\begin{align}Q^{\\pi}(s, a) = r + \\gamma Q^{\\pi}(s', \\pi(s'))\\end{align}$\n",
    "\n",
    "The difference between the two sides of the equality is known as the temporal difference error, $\\delta$:\n",
    "\n",
    "$\\begin{align}\\delta = Q(s, a) - (r + \\gamma \\max_a Q(s', a))\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model\n",
    "---\n",
    "Make a deep learning based policy model, that takes in a state and outputs an action.\n",
    "This model will be an attribute of the Agent we make next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.dense1 = nn.Linear(observation_size, 100)\n",
    "\n",
    "        self.dense2 = nn.Linear(100, 100)\n",
    "        self.dense3 = nn.Linear(100, 100)\n",
    "        # had to hardcode it to 4 becuse action_size is 2\n",
    "        self.dense4 = nn.Linear(100, action_size)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.dense1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.dense2.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.dense3.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.dense4.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dense4(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = torch.tensor(x)\n",
    "        x = self.forward(x)\n",
    "\n",
    "        return torch.argmax(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### DQN Agent\n",
    "----\n",
    "We will be using experience replay memory for training our model.\n",
    "An Agent's memory is as important as its model, and will be another attribute of our agent.\n",
    "Other appropriate attributes are the hyperparameters (gamma, lr, etc.).\n",
    "Give the agent a replay method that trains on a batch from its memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"env = gym.make('CartPole-v1')\\nstate, _ = env.reset()\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"env = gym.make('CartPole-v1')\n",
    "state, _ = env.reset()\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, observation_size, action_size):\n",
    "        self.observation_size = observation_size\n",
    "        self.action_size = action_size\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.model = Model(observation_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        # memory that stores N most new transitions\n",
    "        self.memory = deque([], maxlen=2000)\n",
    "        self.epsilon = 1.0\n",
    "        self.decay_rate = 0.99\n",
    "        # good place to store hyperparameters as attributes\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand > self.epsilon:\n",
    "            return int(self.model.predict(state))\n",
    "        else:\n",
    "            return int(env.action_space.sample())\n",
    "    \n",
    "    def act_optimal(self,state):\n",
    "        return int(self.model.predict(state))\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        # update model based on replay memory\n",
    "        # you might want to make a self.train() helper method\n",
    "        transition_set = random.sample(self.memory, batch_size)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        for transition in transition_set:\n",
    "            self.train(transition)\n",
    "\n",
    "        self.epsilon *= self.decay_rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def train(self, data_set):\n",
    "        state = torch.tensor(data_set[0])\n",
    "        action = torch.tensor(data_set[1])\n",
    "        reward = torch.tensor(data_set[2])\n",
    "        next_state = torch.tensor(data_set[3])\n",
    "        done = torch.tensor(data_set[4])\n",
    "        gamma = 0.9\n",
    "\n",
    "        new_state = reward\n",
    "\n",
    "        if not done:\n",
    "            new_state = reward + gamma * \\\n",
    "                float(torch.max(self.model.forward(next_state)))\n",
    "\n",
    "        prediction = self.model.forward(state)[action]\n",
    "        loss = self.criterion(prediction, new_state)\n",
    "        loss.backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Main Training loop\n",
    "---\n",
    "Make a function that takes and environment and an agent, and runs through $n$ episodes.\n",
    "Remember to call that agent's replay function to learn from its past (once it has a past).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename_out = '5 - DRL_X.txt'\n",
    "\n",
    "def train(envrioment, agent: Agent, episodes=200, batch_size=64):  # train for many games\n",
    "    for e in tqdm(range(episodes)):\n",
    "\n",
    "        if e % 10 == 0:\n",
    "            file = open(filename_out, 'wb')\n",
    "            pickle.dump(agent.model, file)\n",
    "            file.close()\n",
    "            print(\"save\")\n",
    "\n",
    "        state, _ = envrioment.reset()\n",
    "        done = False\n",
    "        total_r = 0\n",
    "        while not done:\n",
    "            # 1. make a move in game.\n",
    "            action = agent.act(state)\n",
    "            # 2. have the agent remember stuff.\n",
    "\n",
    "            next_state, reward, done, _, _ = envrioment.step(action)\n",
    "            total_r += reward\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            # 3. update state\n",
    "            state = next_state\n",
    "            # 4. if we have enough experiences in out memory, learn from a batch with replay.\n",
    "            if len(agent.memory) >= batch_size:\n",
    "                agent.replay(batch_size)\n",
    "\n",
    "    envrioment.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Putting it together\n",
    "---\n",
    "We train an agent on the environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"env = gym.make('CartPole-v1')\\nagent = Agent(env.observation_space.shape[0], env.action_space.n)\\ntrain(env, agent)\\nenv.close()\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"env = gym.make('CartPole-v1')\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.n)\n",
    "train(env, agent)\n",
    "env.close()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mari9\\Desktop\\DeepLearn\\labs\\lab5.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     env\u001b[39m.\u001b[39;49mrender()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mact_optimal(state)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mari9/Desktop/DeepLearn/labs/lab5.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     next_state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\mari9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\core.py:329\u001b[0m, in \u001b[0;36mWrapper.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\n\u001b[0;32m    326\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    327\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Union[RenderFrame, List[RenderFrame]]]:\n\u001b[0;32m    328\u001b[0m     \u001b[39m\"\"\"Renders the environment.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 329\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mari9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     47\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     48\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m     )\n\u001b[1;32m---> 51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mari9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\wrappers\\env_checker.py:55\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m env_render_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     54\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 55\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mrender(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mari9\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:290\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    289\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[1;32m--> 290\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[0;32m    291\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[0;32m    293\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrender_mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "filename_load = '5 - DRL_200.txt'\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "\n",
    "agent = Agent(env.observation_space.shape[0], env.action_space.n)\n",
    "file_open = open(filename_load, 'rb')\n",
    "agent.model = pickle.load(file_open)\n",
    "file_open.close()\n",
    "done = False\n",
    "while_count = 0\n",
    "\n",
    "for episode in range(10):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        action = agent.act_optimal(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        state = next_state\n",
    "        while_count+=1\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional (highly recommended): Atari\n",
    "Adapt your agent to play an Atari game of your choice.\n",
    "https://www.gymlibrary.dev/environments/atari/air_raid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a9cc3d5033db7a69cc0cff3732b1a7e010a67b5ff3241f0937560160e7695f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
